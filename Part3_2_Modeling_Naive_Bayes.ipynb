{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.32 Modeling: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "%store -r X_train\n",
    "%store -r X_test\n",
    "%store -r y_train\n",
    "%store -r y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LogisticRegression related pacakges\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lemmatize and Tokenize rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regularexpression to do lemmatize\n",
    "# use countvectorizer to tokenize, lemmatize, and exclude stopwords\n",
    "# cvec = CountVectorizer(tokenizer=LemmaTokenizer()) \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokenizer = RegexpTokenizer('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "        return [self.wnl.lemmatize(t) for t in tokenizer.tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1474,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Pipeline\n",
    "# using MultinomialNB here\n",
    "pipeline_cv = Pipeline([('cvec', CountVectorizer(tokenizer=LemmaTokenizer())),\n",
    "                        ('mnb', MultinomialNB()) \n",
    "                       ])\n",
    "# Pipeline parameter CountVectorizer\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [300, 500, 1000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [None, 'english']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=4)]: Done 240 out of 240 | elapsed:   10.8s finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "gs_cv = GridSearchCV(pipeline_cv, \n",
    "                     param_grid=pipe_params, \n",
    "                     cv=5,\n",
    "                     verbose=1,\n",
    "                     n_jobs=4)\n",
    "\n",
    "mnb_cv = gs_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 1000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters on the training data:\n",
    "mnb_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score is 0.9613297150610584\n",
      "cross validation score is 0.9355495251017639\n",
      "test score is 0.9410569105691057\n"
     ]
    }
   ],
   "source": [
    "# assign the best estimator to a variable:\n",
    "best_mnb_cv = mnb_cv.best_estimator_\n",
    "# check training score, cross_validation_score and testing score\n",
    "print(f\"training score is {best_mnb_cv.score(X_train, y_train)}\")\n",
    "print(f\"cross validation score is {mnb_cv.best_score_}\")\n",
    "print(f\"test score is {best_mnb_cv.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectorizer - MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Pipeline\n",
    "# using Multinomial here\n",
    "pipeline_tfidf = Pipeline([('tfidf', TfidfVectorizer(tokenizer=LemmaTokenizer())),\n",
    "                           ('mnb', MultinomialNB())\n",
    "                          ])\n",
    "# Pipeline parameter CountVectorizer\n",
    "pipe_params = {\n",
    "    'tfidf__max_features': [300, 500, 1000],\n",
    "    'tfidf__min_df': [2, 3],\n",
    "    'tfidf__max_df': [.9, .95],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__stop_words': [None, 'english']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=4)]: Done 240 out of 240 | elapsed:   11.6s finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "gs_tfidf = GridSearchCV(pipeline_tfidf, \n",
    "                     param_grid=pipe_params, \n",
    "                     cv=5,\n",
    "                     verbose=1,\n",
    "                     n_jobs=4)\n",
    "\n",
    "mnb_tfidf = gs_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__max_df': 0.9,\n",
       " 'tfidf__max_features': 1000,\n",
       " 'tfidf__min_df': 3,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__stop_words': 'english'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters on the training data:\n",
    "mnb_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score is 0.9640434192672999\n",
      "cross validation score is 0.9308005427408412\n",
      "test score is 0.9288617886178862\n"
     ]
    }
   ],
   "source": [
    "# assign the best estimator to a variable:\n",
    "best_mnb_tfidf = mnb_tfidf.best_estimator_\n",
    "# check training score, cross_validation_score and testing score\n",
    "print(f\"training score is {best_mnb_tfidf.score(X_train, y_train)}\")\n",
    "print(f\"cross validation score is {mnb_tfidf.best_score_}\")\n",
    "print(f\"test score is {best_mnb_tfidf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectorizer - GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Pipeline\n",
    "# using Gaussian NB\n",
    "pipeline_tfidf = Pipeline([('tfidf', TfidfVectorizer(tokenizer=LemmaTokenizer())),\n",
    "                           ('toDense', DenseTransformer()),\n",
    "                        ('gnb', GaussianNB()) \n",
    "                       ])\n",
    "# Pipeline parameter CountVectorizer\n",
    "pipe_params = {\n",
    "    'tfidf__max_features': [300, 500, 1000],\n",
    "    'tfidf__min_df': [2, 3],\n",
    "    'tfidf__max_df': [.9, .95],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__stop_words': [None, 'english']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=4)]: Done 240 out of 240 | elapsed:   11.6s finished\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "gs_tfidf = GridSearchCV(pipeline_tfidf, \n",
    "                     param_grid=pipe_params, \n",
    "                     cv=5,\n",
    "                     verbose=1,\n",
    "                     n_jobs=4)\n",
    "\n",
    "gnb_tfidf = gs_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__max_df': 0.9,\n",
       " 'tfidf__max_features': 1000,\n",
       " 'tfidf__min_df': 2,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__stop_words': None}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters on the training data:\n",
    "gnb_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score is 0.9362279511533242\n",
      "cross validation score is 0.8758480325644504\n",
      "test score is 0.8455284552845529\n"
     ]
    }
   ],
   "source": [
    "# assign the best estimator to a variable:\n",
    "best_gnb_tfidf = gnb_tfidf.best_estimator_\n",
    "# check training score, cross_validation_score and testing score\n",
    "print(f\"training score is {best_gnb_tfidf.score(X_train, y_train)}\")\n",
    "print(f\"cross validation score is {gnb_tfidf.best_score_}\")\n",
    "print(f\"test score is {best_gnb_tfidf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check probability to see which feature weights more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since model under CountVectorizer(cv) and tfidf perform similarly, I'll choose cv to do further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the best estimator in gridsearch\n",
    "cvec = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                       max_df=0.9,\n",
    "                       max_features=1000,\n",
    "                       min_df=2,\n",
    "                       ngram_range=(1, 1),\n",
    "                       stop_words='english'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# use CountVectorizer to vectorize the training data\n",
    "X_train_cvec= cvec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a logisticRegression model with lasso\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X_train_cvec as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "cvec_words = pd.DataFrame(X_train_cvec.toarray(), columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prob in the dataframe\n",
    "import numpy as np\n",
    "\n",
    "cv_table_nb = pd.DataFrame(np.exp(mnb.feature_log_prob_), columns = cvec.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spacex</th>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.051804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>launch</th>\n",
       "      <td>0.016618</td>\n",
       "      <td>0.029645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.022458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.014224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dragon</th>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.013176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starship</th>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.012726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heavy</th>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.011080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elon</th>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.010630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crew</th>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.009582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starlink</th>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.009283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1\n",
       "spacex    0.002912  0.051804\n",
       "launch    0.016618  0.029645\n",
       "falcon    0.001199  0.022458\n",
       "twitter   0.004797  0.014224\n",
       "dragon    0.000171  0.013176\n",
       "starship  0.000343  0.012726\n",
       "heavy     0.001542  0.011080\n",
       "elon      0.000343  0.010630\n",
       "crew      0.001885  0.009582\n",
       "starlink  0.000171  0.009283"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check words with high prob for class 1\n",
    "cv_table_nb.sort_values(by=1, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.056707</td>\n",
       "      <td>0.000599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin</th>\n",
       "      <td>0.048655</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>0.043687</td>\n",
       "      <td>0.006288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.007336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shepard</th>\n",
       "      <td>0.017303</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bezos</th>\n",
       "      <td>0.017303</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>launch</th>\n",
       "      <td>0.016618</td>\n",
       "      <td>0.029645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jeff</th>\n",
       "      <td>0.015076</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rocket</th>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.004941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glenn</th>\n",
       "      <td>0.013706</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1\n",
       "blue     0.056707  0.000599\n",
       "origin   0.048655  0.000449\n",
       "new      0.043687  0.006288\n",
       "space    0.018331  0.007336\n",
       "shepard  0.017303  0.000150\n",
       "bezos    0.017303  0.000449\n",
       "launch   0.016618  0.029645\n",
       "jeff     0.015076  0.000449\n",
       "rocket   0.014220  0.004941\n",
       "glenn    0.013706  0.000150"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check words with high prob for class 0\n",
    "cv_table_nb.sort_values(by=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'cv_table_nb' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# store data for visualization\n",
    "%store cv_table_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
